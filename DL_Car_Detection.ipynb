{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41f9367d-28d9-4421-bbae-e63ba4884334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from pycocotools.coco import COCO\n",
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm \n",
    "import json\n",
    "import import_ipynb\n",
    "import nbformat\n",
    "import cv2\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "\n",
    "# Created models/functions for pre-processing data\n",
    "from pre_processing import download_coco_datasets as dcd\n",
    "from pre_processing import filter_coco_datasets as fcd\n",
    "from pre_processing import resize_coco_datasets as rcd\n",
    "from pre_processing import resize_annotations as ra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8067f6f0-a072-40bf-b785-67e1ac80cb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize pixel values to range [0,1]\n",
    "def normalize_min_max(image):\n",
    "    return image / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e360dd68-d179-46c5-93ba-deab579cee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading images as an numpy-array from directory\n",
    "def load_images_from_directory(directory):\n",
    "    images = []\n",
    "    file_list = [f for f in os.listdir(directory) if f.endswith('.jpg') or f.endswith('.jpeg')]\n",
    "    for filename in tqdm(file_list, desc=\"Loading Images\"):\n",
    "        img = cv2.imread(os.path.join(directory, filename))\n",
    "        # Convert to RGB format\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        # Normalize pixel values\n",
    "        img = normalize_min_max(img)\n",
    "        images.append(img)\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70d7be4c-e105-4b43-a8e2-d0e27e6897b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_annotations_from_json(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "    # Parse annotations as needed and return\n",
    "    return tqdm(annotations, desc=\"Loading Annotations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "303e9839-31e7-42d6-8878-23774123aed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de0d951d3c024a75897db249adabbf49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading Images:   0%|          | 0/118287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m pre_processed_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./coco_datasets/resized_images/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m train_2017 \u001b[38;5;241m=\u001b[39m \u001b[43mload_images_from_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpre_processed_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain2017_resized\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m, in \u001b[0;36mload_images_from_directory\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m      4\u001b[0m file_list \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(directory) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpeg\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m tqdm(file_list, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading Images\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m----> 6\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     images\u001b[38;5;241m.\u001b[39mappend(img)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(images)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Path to pre-processed data directory\n",
    "pre_processed_data = './coco_datasets/resized_images/'\n",
    "\n",
    "# loading images as a numpy-array\n",
    "train_data = load_images_from_directory(os.path.join(pre_processed_data, 'train2017_resized'))\n",
    "test_data = load_images_from_directory(os.path.join(pre_processed_data, 'test2017_resized'))\n",
    "val_data = load_images_from_directory(os.path.join(pre_processed_data, 'val2017_resized'))\n",
    "\n",
    "\n",
    "# Loading size adjusted annotations from directory\n",
    "annotations_dir = './coco_datasets/adjusted_annotations/\n",
    "train_annotations = load_annotations_from_json(os.path.join(annotations_dir, 'instances_train2017.json'))\n",
    "val_annotations = load_annotations_from_json(os.path.join(annotations_dir, 'instances_val2017.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cf71ee-d674-479f-baae-35f5c86c4a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 244x244 pixels, 3 channels (RGB)\n",
    "input_shape = (224, 224, 3)\n",
    "num_classes = 5  # xmin, ymin, xmax, ymax, class\n",
    "\n",
    "# Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers to the model\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes))  # Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7399fb93-96b4-4596-a1c4-3547c90d614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model, using adam as optimizer and mean square error as loss function\n",
    "model.compile(optimizer='adam', loss='mse') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff460ac-625e-4106-99ce-a72836b4f035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a log directory for visualization logs\n",
    "# tensorboard --logdir logs/fit    # use in command to get url link for logs\n",
    "\n",
    "log_dir = \"logs/fit/\"\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1003e3ed-452e-4ffc-a244-d69b6992feda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model with 10 epochs and a batch size of 32\n",
    "model.fit(train_data, train_annotations, validation_data=(val_data, val_annotations), epochs=10, batch_size=32, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6434e6-94a8-4f9d-ad2c-61d7227b3d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model on the test data\n",
    "model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9034ead-4636-4c83-906f-51ed2cdc0885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "model.save('car_detector_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061cda43-06dd-42e6-b024-6baed090f5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model('car_detector_model.h5')\n",
    "predictions = loaded_model.predict(new_images) # REMOVE new_images for other data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
