{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41f9367d-28d9-4421-bbae-e63ba4884334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from pycocotools.coco import COCO\n",
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm \n",
    "import json\n",
    "import ipywidgets\n",
    "import nbformat\n",
    "import cv2\n",
    "import functools\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "\n",
    "# Created models/functions for pre-processing data\n",
    "from pre_processing import download_coco_datasets as dcd\n",
    "from pre_processing import filter_coco_datasets as fcd\n",
    "from pre_processing import resize_coco_datasets as rcd\n",
    "from pre_processing import resize_annotations as ra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c9237e8-3d31-43ba-b292-594c64ea8244",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87ce3a54-55de-4df3-a3b4-d330788bc343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8067f6f0-a072-40bf-b785-67e1ac80cb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize pixel values to range [0,1]\n",
    "def normalize_min_max(image):\n",
    "    return image / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70d7be4c-e105-4b43-a8e2-d0e27e6897b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_annotations_from_json(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "    \n",
    "    # Assuming annotations is a list of items to iterate over\n",
    "    for annotation in tqdm(annotations, desc=\"Loading Annotations\"):\n",
    "        # Replace process_annotation with your actual parsing logic\n",
    "        # For example, print each annotation\n",
    "        print(annotation)\n",
    "    \n",
    "    return annotations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0d4bdc5-35e9-4a22-9043-54bfeb6626cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_and_annotation(image_dir, annotation_dir):\n",
    "    image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg') or f.endswith('.jpeg')]\n",
    "    annotations = load_annotations_from_json(os.path.join(annotation_dir, 'instances_train2017.json'))\n",
    "    for filename in tqdm(image_files, desc=\"Loading Images\"):\n",
    "        img = cv2.imread(os.path.join(image_dir, filename))\n",
    "        if img is None:\n",
    "            continue\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        annotation_file = os.path.splitext(filename)[0] + '.json'\n",
    "        annotation_path = os.path.join(annotation_dir, annotation_file)\n",
    "        if not os.path.exists(annotation_path):\n",
    "            continue\n",
    "        with open(annotation_path, 'r') as f:\n",
    "            annotation = json.load(f)\n",
    "        yield img, annotation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "303e9839-31e7-42d6-8878-23774123aed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d82e1b7a83480cb07308b5959b722c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading Annotations:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info\n",
      "licenses\n",
      "images\n",
      "annotations\n",
      "categories\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "408a2a94996740a1b95f8ac4a4bb35f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading Images:   0%|          | 0/118287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 16\u001b[0m\n\u001b[0;32m     12\u001b[0m test_image_annotation_generator \u001b[38;5;241m=\u001b[39m generate_image_and_annotation(\n\u001b[0;32m     13\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(pre_processed_data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest2017_resized\u001b[39m\u001b[38;5;124m'\u001b[39m), annotations_dir)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Collect the generated data into lists\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m train_images, train_annotations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mtrain_image_annotation_generator)\n\u001b[0;32m     17\u001b[0m val_images, val_annotations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mval_image_annotation_generator)\n\u001b[0;32m     18\u001b[0m test_images, test_annotations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mtest_image_annotation_generator)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "# Path to pre-processed data directory\n",
    "pre_processed_data = './coco_datasets/resized_images/'\n",
    "annotations_dir = './coco_datasets/adjusted_annotations/'\n",
    "\n",
    "\n",
    "\n",
    "# Load images and annotations using the generator\n",
    "train_image_annotation_generator = generate_image_and_annotation(\n",
    "    os.path.join(pre_processed_data, 'train2017_resized'), annotations_dir)\n",
    "val_image_annotation_generator = generate_image_and_annotation(\n",
    "    os.path.join(pre_processed_data, 'val2017_resized'), annotations_dir)\n",
    "test_image_annotation_generator = generate_image_and_annotation(\n",
    "    os.path.join(pre_processed_data, 'test2017_resized'), annotations_dir)\n",
    "\n",
    "# Collect the generated data into lists\n",
    "train_images, train_annotations = zip(*train_image_annotation_generator)\n",
    "val_images, val_annotations = zip(*val_image_annotation_generator)\n",
    "test_images, test_annotations = zip(*test_image_annotation_generator)\n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "train_images = np.array(train_images)\n",
    "val_images = np.array(val_images)\n",
    "test_images = np.array(test_images)\n",
    "\n",
    "# Example usage:\n",
    "for img, annotation in zip(train_images, train_annotations):\n",
    "    # Process each training image and annotation here\n",
    "    pass\n",
    "\n",
    "for img, annotation in zip(val_images, val_annotations):\n",
    "    # Process each validation image and annotation here\n",
    "    pass\n",
    "\n",
    "for img, annotation in zip(test_images, test_annotations):\n",
    "    # Process each test image and annotation here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cf71ee-d674-479f-baae-35f5c86c4a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 244x244 pixels, 3 channels (RGB)\n",
    "input_shape = (224, 224, 3)\n",
    "num_classes = 5  # xmin, ymin, xmax, ymax, class\n",
    "\n",
    "# Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers to the model\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes))  # Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7399fb93-96b4-4596-a1c4-3547c90d614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model, using adam as optimizer and mean square error as loss function\n",
    "model.compile(optimizer='adam', loss='mse') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f26c6b-245b-43dd-8349-e2d67c419938",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff460ac-625e-4106-99ce-a72836b4f035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a log directory for visualization logs\n",
    "# tensorboard --logdir logs/fit    # use in command to get url link for logs\n",
    "\n",
    "log_dir = \"logs/fit/\"\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c89b7f-b145-441f-a1e5-e59743a8c44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TQDMCallback(Callback):\n",
    "    def __init__(self, total_epochs):\n",
    "        super(TQDMCallback, self).__init__()\n",
    "        self.total_epochs = total_epochs\n",
    "        self.progress_bar = tqdm(total=self.total_epochs)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.progress_bar.update(1)\n",
    "        self.progress_bar.set_description(f\"Epoch {epoch+1}/{self.total_epochs}\")\n",
    "        self.progress_bar.set_postfix(logs)\n",
    "        self.progress_bar.refresh()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00440da6-77ce-4faf-a699-2cbce679de59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the TQDMCallback\n",
    "tqdm_callback = TQDMCallback(total_epochs=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c38184-1313-4838-a598-41f5cedb272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32  # Define batch size\n",
    "\n",
    "# Create data generators\n",
    "train_generator = data_generator(train_image_loader, train_annotations, batch_size)\n",
    "val_generator = data_generator(val_image_loader, val_annotations, batch_size)\n",
    "\n",
    "\n",
    "# Calculate actual steps per epoch\n",
    "steps_per_epoch = len(train_annotations) // batch_size\n",
    "\n",
    "# Adjust validation steps\n",
    "validation_steps = len(val_annotations) // batch_size\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753c6145-f1c2-4be1-aa7b-2d414f7e3fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the generator and print batches of data\n",
    "for batch_images, batch_annotations in train_generator:\n",
    "    print(\"Batch Images:\", batch_images)\n",
    "    print(\"Batch Annotations:\", batch_annotations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4321b64-5706-4b8f-99dd-52eb0114e169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model with 10 epochs and the data generators\n",
    "model.fit(train_generator,\n",
    "          steps_per_epoch=steps_per_epoch,\n",
    "          validation_data=val_generator,\n",
    "          validation_steps=validation_steps,\n",
    "          epochs=10,\n",
    "          callbacks=[tqdm_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6434e6-94a8-4f9d-ad2c-61d7227b3d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model on the test data\n",
    "model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9034ead-4636-4c83-906f-51ed2cdc0885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "model.save('car_detector_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061cda43-06dd-42e6-b024-6baed090f5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model('car_detector_model.h5')\n",
    "predictions = loaded_model.predict(new_images) # REMOVE new_images for other data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
